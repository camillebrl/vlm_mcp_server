"""Gestionnaire singleton pour le modÃ¨le InternVL3 avec chargement/dÃ©chargement dynamique."""

import gc
import logging
import time
from threading import Lock
from typing import Optional

import torch

from .vlms_model import VLMSModel

logger = logging.getLogger(__name__)


class VLMSModelManager:
    """Gestionnaire singleton pour le modÃ¨le InternVL3 avec chargement/dÃ©chargement dynamique."""

    _instance: Optional["VLMSModelManager"] = None
    _lock = Lock()

    def __new__(cls, *args, **kwargs):
        """CrÃ©e ou retourne l'instance unique du gestionnaire.

        ImplÃ©mente le pattern Singleton pour s'assurer qu'une seule instance
        du gestionnaire de modÃ¨le existe dans l'application.

        Returns:
            L'instance unique de VLMSModelManager
        """
        if not cls._instance:
            with cls._lock:
                if not cls._instance:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        # Ne rÃ©initialiser que si c'est la premiÃ¨re fois
        # Ne rÃ©initialiser que si c'est la premiÃ¨re fois
        if not hasattr(self, "_initialized"):
            self._initialized = True
            self._model: VLMSModel | None = None
            self._model_path: str | None = None
            self._last_used: float = 0
            self._load_lock = Lock()
            self._reference_count = 0
            self._unload_delay = 0  # DÃ©lai en secondes avant dÃ©chargement (0 = immÃ©diat)

    def acquire(self, model_path: str = "OpenGVLab/InternVL3-1B") -> VLMSModel:
        """Acquiert une rÃ©fÃ©rence au modÃ¨le et le charge si nÃ©cessaire."""
        with self._load_lock:
            self._reference_count += 1
            self._last_used = time.time()

            # Charger le modÃ¨le si pas dÃ©jÃ  chargÃ© ou si le chemin a changÃ©
            if self._model is None or self._model_path != model_path:
                self._load_model(model_path)

            # Ã€ ce stade, _model ne devrait jamais Ãªtre None
            if self._model is None:
                raise RuntimeError("Ã‰chec du chargement du modÃ¨le")

            return self._model

    def release(self):
        """LibÃ¨re une rÃ©fÃ©rence au modÃ¨le."""
        with self._load_lock:
            self._reference_count = max(0, self._reference_count - 1)
            self._last_used = time.time()

            # DÃ©charger immÃ©diatement si aucune rÃ©fÃ©rence et pas de dÃ©lai
            if self._reference_count == 0 and self._unload_delay == 0:
                self._unload_model()

    def check_and_unload(self):
        """VÃ©rifie si le modÃ¨le doit Ãªtre dÃ©chargÃ© (appelÃ© pÃ©riodiquement)."""
        with self._load_lock:
            if (
                self._model is not None
                and self._reference_count == 0
                and time.time() - self._last_used > self._unload_delay
            ):
                self._unload_model()

    def _load_model(self, model_path: str):
        """Charge le modÃ¨le en mÃ©moire."""
        # DÃ©charger l'ancien modÃ¨le si nÃ©cessaire
        if self._model is not None:
            self._unload_model()

        try:
            logger.info(f"ðŸ“¦ Chargement du modÃ¨le {model_path}...")
            start_time = time.time()

            # Nettoyage mÃ©moire avant chargement
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()

            # CrÃ©er et charger le modÃ¨le
            self._model = VLMSModel(model_path=model_path)
            self._model_path = model_path

            load_time = time.time() - start_time
            logger.info(f"âœ… ModÃ¨le chargÃ© en {load_time:.2f}s")

            # Afficher l'utilisation mÃ©moire
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated(0) / 1024**3
                reserved = torch.cuda.memory_reserved(0) / 1024**3
                logger.info(f"ðŸ“Š MÃ©moire GPU: AllouÃ©e={allocated:.2f}GB, RÃ©servÃ©e={reserved:.2f}GB")

        except Exception as e:
            logger.error(f"âŒ Erreur lors du chargement du modÃ¨le: {e}")
            self._model = None
            self._model_path = None
            raise

    def _unload_model(self):
        """DÃ©charge le modÃ¨le de la mÃ©moire GPU."""
        if self._model is None:
            return

        try:
            logger.info("ðŸ§¹ DÃ©chargement du modÃ¨le InternVL3...")

            # Appeler la mÃ©thode cleanup du modÃ¨le
            if hasattr(self._model, "cleanup"):
                self._model.cleanup()

            # Supprimer les rÃ©fÃ©rences
            del self._model
            self._model = None
            self._model_path = None

            # Forcer le garbage collection Python
            logger.info("   -> Garbage collection...")
            gc.collect()

            # Vider le cache CUDA de maniÃ¨re agressive
            if torch.cuda.is_available():
                logger.info("   -> Vidage agressif du cache CUDA...")

                # Plusieurs passes de nettoyage
                for _ in range(3):
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()

                # Essayer de rÃ©initialiser le contexte CUDA si possible
                try:
                    # RÃ©initialiser l'allocateur de mÃ©moire (PyTorch >= 1.10)
                    if hasattr(torch.cuda, "memory._set_allocator_settings"):
                        torch.cuda.memory._set_allocator_settings("")

                    # Forcer la libÃ©ration de toute la mÃ©moire rÃ©servÃ©e
                    torch.cuda.reset_peak_memory_stats()
                    torch.cuda.reset_accumulated_memory_stats()

                except Exception as e:
                    logger.debug(f"MÃ©thodes de reset CUDA non disponibles: {e}")

                # Afficher la mÃ©moire aprÃ¨s nettoyage
                after_allocated = torch.cuda.memory_allocated(0) / 1024**3
                after_reserved = torch.cuda.memory_reserved(0) / 1024**3

                logger.info("âœ… ModÃ¨le dÃ©chargÃ©:")
                logger.info(f"ðŸ“Š MÃ©moire GPU finale: AllouÃ©e={after_allocated:.2f}GB, RÃ©servÃ©e={after_reserved:.2f}GB")

                if after_reserved > 0.5:  # Plus de 500MB encore rÃ©servÃ©s
                    logger.warning(
                        f"âš ï¸ {after_reserved:.2f}GB de mÃ©moire encore rÃ©servÃ©e. "
                        "PyTorch garde la mÃ©moire en cache. DÃ©finissez PYTORCH_NO_CUDA_MEMORY_CACHING=1 "
                        "pour forcer la libÃ©ration complÃ¨te."
                    )
            else:
                logger.info("âœ… ModÃ¨le dÃ©chargÃ©")

        except Exception as e:
            logger.error(f"âŒ Erreur lors du dÃ©chargement: {e}")
            # MÃªme en cas d'erreur, s'assurer que les rÃ©fÃ©rences sont nulles
            self._model = None
            self._model_path = None

            # Tenter quand mÃªme un nettoyage basique
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    def force_unload(self):
        """Force le dÃ©chargement immÃ©diat du modÃ¨le, ignore les rÃ©fÃ©rences."""
        with self._load_lock:
            logger.info("âš ï¸ DÃ©chargement forcÃ© du modÃ¨le InternVL3...")
            self._reference_count = 0
            self._unload_model()

    @property
    def is_loaded(self) -> bool:
        """VÃ©rifie si le modÃ¨le est chargÃ©."""
        return self._model is not None

    def get_model_info(self) -> dict:
        """Retourne des informations sur le modÃ¨le."""
        info = {
            "loaded": self.is_loaded,
            "model_path": self._model_path,
            "reference_count": self._reference_count,
            "last_used": (time.time() - self._last_used if self._last_used > 0 else None),
            "unload_delay": self._unload_delay,
        }

        if self.is_loaded and torch.cuda.is_available():
            info["gpu_memory_allocated"] = f"{torch.cuda.memory_allocated(0) / 1024**3:.2f} GB"
            info["gpu_memory_reserved"] = f"{torch.cuda.memory_reserved(0) / 1024**3:.2f} GB"

        return info


# Fonction utilitaire pour obtenir le gestionnaire singleton
def get_vlms_manager() -> VLMSModelManager:
    """Retourne l'instance singleton du gestionnaire InternVL3."""
    return VLMSModelManager()
